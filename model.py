import itertools
from csv import DictReader
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import RandomForestClassifier as RF
from xgboost_multi import XGBC
from sklearn import cross_validation
from sklearn.cross_validation import StratifiedKFold as KFold
from sklearn.metrics import log_loss
import numpy as np
import pandas as pd
import pickle
import os
import matplotlib.pyplot as plt
from matplotlib.legend_handler import HandlerLine2D

data_folder = "Data"
train_path = "Data/train"
test_path = "Data/test"
csv_path = "Data/trainLabels.csv"

filter_number = 1.25


def plot_confusion_matrix(cm,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, cmap=cmap,shape=(4,4))
    plt.title(title)
    plt.colorbar()

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 12.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

def get_model_list():
    model_list = []
    for num_round in [200]:
        for max_depth in [2]:
            for eta in [0.25]:
                for min_child_weight in [2]:
                    for col_sample in [1]:
                        model_list.append((XGBC(num_round=num_round, max_depth=max_depth, eta=eta,
                                                min_child_weight=min_child_weight, colsample_bytree=col_sample),
                                           'xgb_tree_%i_depth_%i_lr_%f_child_%i_col_sample_%i' % (
                                           num_round, max_depth, eta, min_child_weight, col_sample)))

    print (model_list)
    return model_list

def filter_features(features, features_importance, train, test):

    features_dict = {}
    for i in range(len(features_importance)):
        features_dict[features[i]] = features_importance[i]
    mean = np.mean(features_importance)
    filter_size = mean * filter_number
    for f in features:
        if(features_dict[f] < filter_size):
            del train[f]
            del test[f]

    return train, test



def gen_data(i = 0):
    grams_train = pd.read_csv(data_folder +'/train_data_750.csv')
    grams_test = pd.read_csv(data_folder +'/test_data_750.csv')

    mine_labels = pd.read_csv(csv_path)
    train_files = os.listdir(train_path)
    train_files = [os.path.splitext(x)[0] for x in train_files]

    mine_labels = mine_labels[mine_labels['Id'].isin(train_files)]
    train = pd.merge(mine_labels, grams_train, on='Id')
    train_labels = train.Class
    #X = grams_train
    #del X['Id']
    X = del_Id_and_Class_Columns(grams_train, Class=False)
    clf_se = RF(n_estimators=500, n_jobs=-1, random_state=0)
    clf_se.fit(X, train_labels)
    features = grams_train.dtypes.index
    features_importance = clf_se.feature_importances_
    train, test = filter_features(features, features_importance, train, grams_test)

    return train, test



def get_y(x):
    for row in DictReader(open(csv_path)):
        if row['Id'] == x:
            return int(row['Class'])

    return -1

def replace_labels_9_to_5(labels):
    for i in range(len(labels)):
        if labels[i] == 9:
            labels[i] = 5
    return labels


def gen_semi_label(model):
    # read in data
    print ("inside gen_semi_label")
    train, test = gen_data()
    X, X_test = train.copy(), test.copy()
    labels = np.array(X.Class)  # for the purpose of using multilogloss fun.
    X = del_Id_and_Class_Columns(X)
    id_test = X_test.Id
    X_test = del_Id_and_Class_Columns(X_test,Class=False)
    X_test, X = X_test.as_matrix(),X.as_matrix()
    labels = replace_labels_9_to_5(labels)

    clf, clf_name = model
    clf.fit(X, labels)

    pred = clf.predict_proba(X_test)
    pred = pred.reshape(X_test.shape[0], 9)
    pred = np.column_stack((id_test, pred))
    submission = pd.DataFrame(pred, columns=['Id'] + ['Prediction%i' % i for i in range(1, 10)])
    submission = submission.apply(pd.to_numeric, errors='ignore')

    semi_labels = np.array([int(x[-1]) for x in submission.ix[:, 1:].idxmax(1)])
    semi_labels = np.column_stack((id_test, semi_labels))
    semi_labels = pd.DataFrame(semi_labels, columns=['Id', 'Class'])
    semi_labels = semi_labels.apply(pd.to_numeric, errors='ignore')

    test = pd.merge(test, semi_labels, on='Id', how='inner')
    return train, test


def cross_validate(model_list):
    # read in data
    print ("inside cross_validation - read data and prepare modelling...")
    train, test = gen_semi_label(model_list[0])
    X = train.copy()
    Id = X.Id
    labels = np.array(X.Class)  # for the purpose of using multilogloss fun.
    X = del_Id_and_Class_Columns(X)
    X = X.as_matrix()
    X_test = test
    id_test = X_test.Id
    labels_test = np.array(X_test.Class)
    X_test = del_Id_and_Class_Columns(X_test)
    X_test = X_test.as_matrix()

    kf = KFold(labels, n_folds=1)  # 4 folds
    for j, (clf, clf_name) in enumerate(model_list):
        stack_train = np.zeros((len(Id), 9))  # 9 classes.
        for i, (train_fold, validate) in enumerate(kf):
            X_train, X_validate, labels_train, labels_validate = X[train_fold, :], X[validate, :], labels[train_fold], \
                                                                 labels[validate]
            X_train = np.concatenate((X_train, X_test))
            labels_train = np.concatenate((labels_train, labels_test))

            labels_train = replace_labels_9_to_5(labels_train)

            clf.fit(X_train, labels_train)
            stack_train[validate] = clf.predict_proba(X_validate)
            # print "finish one fold..."
        labels = replace_labels_9_to_5(labels)
        print("multiclass_log_loss : {}".format(multiclass_log_loss(labels, stack_train)))

def del_Id_and_Class_Columns(data,id=True, Class=True):
    if(id):
        del data['Id']
    if(Class):
        del data['Class']
    return data


def semi_learning(model_list):
    # read in data
    print("read data and prepare modelling...")
    train, test = gen_semi_label(model_list[0])
    labels = np.array(train.Class)  # for the purpose of using multilogloss fun.
    X = del_Id_and_Class_Columns(train)
    id_test = test.Id
    labels_test = np.array(test.Class)
    X_test = del_Id_and_Class_Columns(test)
    X, X_test = X.as_matrix(), X_test.as_matrix()

    kf = KFold(labels_test, n_folds=2)  # 2 folds
    maxpred = np.zeros(len(id_test))
    real = np.zeros(len(id_test))
    for j, (clf, clf_name) in enumerate(model_list):
        stack_train = np.zeros((len(id_test), 9))  # 9 classes.
        for i, (train_fold, validate) in enumerate(kf):
            X_train, X_validate, labels_train, labels_validate = X_test[train_fold, :], X_test[validate, :], \
                                                                 labels_test[train_fold], labels_test[validate]
            X_train = np.concatenate((X, X_train))
            labels_train = np.concatenate((labels, labels_train))
            labels_train = replace_labels_9_to_5(labels_train)
            clf.fit(X_train, labels_train)
            stack_train[validate] = clf.predict_proba(X_validate).reshape(X_validate.shape[0], 9)
        maxpred = np.zeros(len(id_test))
        real = np.zeros(len(id_test))
        for i in range(len(id_test)):
            maxpred[i] = np.argmax(stack_train[i])
            real[i] = get_y(id_test[i])
            if(real[i] == 9):
                real[i] = 5
            if(real[i] != maxpred[i]):
                print(id_test[i])

        confusion_mat = confusion_matrix(real, maxpred)
        print("confusion matrix: \n{}".format(confusion_mat))

        plot_confusion_matrix(confusion_mat, normalize=True,
                              title='Confusion matrix')

        pred = np.column_stack((id_test, stack_train, maxpred))

        submission = pd.DataFrame(pred, columns=['Id'] + ['Prediction%i' % i for i in range(0, 9)]+ ['Class Prediction'])
        submission = submission.apply(pd.to_numeric, errors='ignore')
        submission.to_csv('submission.csv', index=False)
    return maxpred, real



def multiclass_log_loss(y_true, y_pred, eps=1e-15):
    """Multi class version of Logarithmic Loss metric.
    https://www.kaggle.com/wiki/MultiClassLogLoss

    Parameters
    ----------
    y_true : array, shape = [n_samples]
            true class, intergers in [0, n_classes - 1)
    y_pred : array, shape = [n_samples, n_classes]

    Returns
    -------
    loss : float
    """
    predictions = np.clip(y_pred, eps, 1 - eps)

    # normalize row sums to 1
    predictions /= predictions.sum(axis=1)[:, np.newaxis]

    actual = np.zeros(y_pred.shape)
    n_samples = actual.shape[0]
    actual[np.arange(n_samples), y_true.astype(int)] = 1
    vectsum = np.sum(actual * np.log(predictions))
    loss = -1.0 / n_samples * vectsum
    return loss

def calc_and_print_success_rate(maxpred, real):
    total_success, benign_success, benign_fail = 0,0,0
    for y_hat,y in zip(maxpred,real):
        if(y_hat == y):
            total_success +=1
            if(y_hat == 0):
                benign_success +=1
        else:
            if(y_hat ==0 or y == 0):
               benign_fail+=1

    total_size = len(maxpred)
    total_benign = benign_success + benign_fail

    print("total success is: {}, from {}, in percent: ({:.0f}%) ".format(total_success, total_size, 100.*total_success / total_size))
    print("benign success is: {}, from {}, in percent: ({:.0f}%) ".format(benign_success, total_benign,
                                                                  100.*benign_success / total_benign))
    print("benign fail is: {}, from {}, in percent: ({:.0f}%) ".format(benign_fail, total_benign,
                                                                  100.*benign_fail / total_benign))

    return 100.*total_success / total_size

def calc_and_plot_the_filter_number(model_list):
    success_dict = {}
    for i in range(100,200,5):
        filter_number = i/100
        print("model list: ")
        print(model_list)
        succ = calc_and_print_success_rate(maxpred, real)
        success_dict[filter_number] = succ
        #print "ALL DONE!!!"
        print(i)
    print(success_dict)
    label1, = plt.plot(success_dict.keys(), success_dict.values(), "b-", label="Training average loss per epoch")
    plt.legend(handler_map={label1: HandlerLine2D(numpoints=4)})
    plt.show()

def update_pathes():
    global data_folder, train_path, test_path, csv_path
    with open('path.txt', 'r') as f:
        data_folder = f.readline().replace('\r','').replace('\n','')
        train_path = f.readline().replace('\r','').replace('\n','')
        test_path = f.readline().replace('\r','').replace('\n','')
        csv_path = f.readline().replace('\r','').replace('\n','')


if __name__ == '__main__':
    update_pathes()
    model_list = get_model_list()
    maxpred, real = semi_learning(model_list)
    succ = calc_and_print_success_rate(maxpred, real)



