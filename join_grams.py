import heapq
import pickle
import math
from csv import DictReader
import glob
import os
import csv

data_folder = "Data"
train_path = "Data/train"
test_path = "Data/test"
csv_path = "trainLabels.csv"

def join_ngrams(num=100000):
    dict_all = dict()
    for c in range(0, 10):
        if os.path.isfile('gram/ngram_%i_top%i' % (c, num)):
            print("merging %i out of 9" % c)
            heap = pickle.load(open('gram/ngram_%i_top%i' % (c, num), 'rb'))
            while heap:
                count, gram = heapq.heappop(heap)
                if gram not in dict_all:
                    dict_all[gram] = [0] * 10
                dict_all[gram][c] = count
    return dict_all

# load data
def num_instances(path, label):
    p = 0
    n = 0
    dict = DictReader(open(path))
    for row in dict:
        if row['Class'] == str(label):
            p += 1
        else:
            n += 1
    return p, n


def entropy(p, n):
    p_ratio = float(p) / (p + n)
    n_ratio = float(n) / (p + n)
    return -p_ratio * math.log(p_ratio) - n_ratio * math.log(n_ratio)


def info_gain(p0, n0, p1, n1, p, n):
    return entropy(p, n) - float(p0 + n0) / (p + n) * entropy(p0, n0) - float(p1 + n1) / (p + n) * entropy(p1, n1)


def Heap_gain(p, n, class_label, dict_all, num_features=750, gain_minimum_bar=-100000):
    print ("inside Heap_gain")
    heap = [(gain_minimum_bar, 'gain_bar')] * num_features
    root = heap[0]
   # print (root)
    for gram, count_list in dict_all.iteritems():
        p1 = count_list[class_label]
        n1 = sum(count_list[:(class_label)] + count_list[class_label + 1:])
        p0, n0 = p - p1, n - n1
        if p1 * p0 * n1 * n0 * p * n != 0:
            gain = info_gain(p0, n0, p1, n1, p, n)
            if gain > root[0]:
                root = heapq.heapreplace(heap, (gain, gram))
    # return heap
    return [i[1] for i in heap]


def gen_df(features_all, train=True, verbose=False, N=4):
    yield ['Id'] + features_all  # yield header
    if train == True:
        ds = train_path
    else:
        ds = test_path

   # print((os.path.join(ds, "*.bytes")))
    directory_names = list(set(glob.glob(os.path.join(ds, "*.bytes"))))

    #print("Directory names : ".format(directory_names))
    #print("Directory_names.len is: ".format(directory_names.__len__()))

    iter = 1
    for f in directory_names:
        f_id = f.split('/')[-1].split('.')[0]
        if verbose:
            print("doing {} iteration: {}".format(f_id, iter))
            iter += 1
        one_list = []
        with open("%s/%s.bytes" % (ds, f_id), 'rb') as read_file:
            for line in read_file:
                one_list += line.rstrip().split(" ")[1:]
        grams_string = [''.join(one_list[i:i + N]) for i in xrange(len(one_list) - N)]
        # build a dict for looking up

        grams_dict = dict()
        for gram in grams_string:
            if gram not in grams_dict:
                grams_dict[gram] = 1

        binary_features = []
        for feature in features_all:
            if feature in grams_dict:
                binary_features.append(1)
            else:
                binary_features.append(0)
        del grams_string
        # binary_features - a binary array when binary_features[i] = 0 -> feature i is not in this file..
        '''
        ## instead of binary features, do count
        grams_dict = dict()
        for gram in grams_string:
            if gram not in grams_dict:
                grams_dict[gram] = 1
            else:
                grams_dict[gram] += 1 

        binary_features = []
        for feature in features_all:
            if feature in grams_dict:
                binary_features.append(grams_dict[feature])
            else:
                binary_features.append(0)
        del grams_string        
        '''
        yield [f_id] + binary_features

def update_pathes():
    global data_folder, train_path, test_path, csv_path
    with open('path.txt', 'r') as f:
        data_folder = f.readline().replace('\r','').replace('\n','')
        train_path = f.readline().replace('\r','').replace('\n','')
        test_path = f.readline().replace('\r','').replace('\n','')
        csv_path = f.readline().replace('\r','').replace('\n','')


def write_data_750_csv(data, path):
    with open(path, 'wb') as outfile:
        wr = csv.writer(outfile, delimiter=',', quoting=csv.QUOTE_ALL)
        for row in data:
            wr.writerow(row)



if __name__ == '__main__':
    dict_all = join_ngrams()
    update_pathes()
    # dict_all is a dictionary looks like this : {'gram', array[10]}
    # example - {...,'6B57C771': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],..}
    # when array[i] is how many time we saw the gram in class i
    features_all = []
    for i in range(0, 10):
        p, n = num_instances(csv_path, i)
        if p > 0:
            # p is how many instances we have in class i and n is how many instances we have not from class i
           # print ("p is: {}    n is: {}    i is: {}".format(p,n,i) )
            features_all += Heap_gain(p, n, i, dict_all)  # 750 * 9
            # the list of all the features in class i


    train_data = gen_df(features_all, train=True, verbose=True)
    write_data_750_csv(train_data, data_folder + '/train_data_750.csv')

    test_data = gen_df(features_all, train=False, verbose=True)
    write_data_750_csv(test_data, data_folder + '/test_data_750.csv')

    print ("DONE 4 gram features!")
